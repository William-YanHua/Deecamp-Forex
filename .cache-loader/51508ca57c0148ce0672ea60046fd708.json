{"remainingRequest":"/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/ts-loader/index.js??ref--4-1!/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/remark-parse/lib/tokenize/text.js","dependencies":[{"path":"/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/remark-parse/lib/tokenize/text.js","mtime":1516191690000},{"path":"/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/cache-loader/dist/cjs.js","mtime":499162500000},{"path":"/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/ts-loader/index.js","mtime":499162500000}],"contextDependencies":[],"result":["'use strict';\nmodule.exports = text;\nfunction text(eat, value, silent) {\n    var self = this;\n    var methods;\n    var tokenizers;\n    var index;\n    var length;\n    var subvalue;\n    var position;\n    var tokenizer;\n    var name;\n    var min;\n    var now;\n    if (silent) {\n        return true;\n    }\n    methods = self.inlineMethods;\n    length = methods.length;\n    tokenizers = self.inlineTokenizers;\n    index = -1;\n    min = value.length;\n    while (++index < length) {\n        name = methods[index];\n        if (name === 'text' || !tokenizers[name]) {\n            continue;\n        }\n        tokenizer = tokenizers[name].locator;\n        if (!tokenizer) {\n            eat.file.fail('Missing locator: `' + name + '`');\n        }\n        position = tokenizer.call(self, value, 1);\n        if (position !== -1 && position < min) {\n            min = position;\n        }\n    }\n    subvalue = value.slice(0, min);\n    now = eat.now();\n    self.decode(subvalue, now, function (content, position, source) {\n        eat(source || content)({\n            type: 'text',\n            value: content\n        });\n    });\n}\n",{"version":3,"file":"/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/remark-parse/lib/tokenize/text.js","sourceRoot":"","sources":["/run/media/williamyh/新加卷/project/Deecamp-project/system/Web/node_modules/remark-parse/lib/tokenize/text.js"],"names":[],"mappings":"AAAA,YAAY,CAAC;AAEb,MAAM,CAAC,OAAO,GAAG,IAAI,CAAC;AAEtB,SAAS,IAAI,CAAC,GAAG,EAAE,KAAK,EAAE,MAAM;IAC9B,IAAI,IAAI,GAAG,IAAI,CAAC;IAChB,IAAI,OAAO,CAAC;IACZ,IAAI,UAAU,CAAC;IACf,IAAI,KAAK,CAAC;IACV,IAAI,MAAM,CAAC;IACX,IAAI,QAAQ,CAAC;IACb,IAAI,QAAQ,CAAC;IACb,IAAI,SAAS,CAAC;IACd,IAAI,IAAI,CAAC;IACT,IAAI,GAAG,CAAC;IACR,IAAI,GAAG,CAAC;IAGR,IAAI,MAAM,EAAE;QACV,OAAO,IAAI,CAAC;KACb;IAED,OAAO,GAAG,IAAI,CAAC,aAAa,CAAC;IAC7B,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC;IACxB,UAAU,GAAG,IAAI,CAAC,gBAAgB,CAAC;IACnC,KAAK,GAAG,CAAC,CAAC,CAAC;IACX,GAAG,GAAG,KAAK,CAAC,MAAM,CAAC;IAEnB,OAAO,EAAE,KAAK,GAAG,MAAM,EAAE;QACvB,IAAI,GAAG,OAAO,CAAC,KAAK,CAAC,CAAC;QAEtB,IAAI,IAAI,KAAK,MAAM,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,EAAE;YACxC,SAAS;SACV;QAED,SAAS,GAAG,UAAU,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC;QAErC,IAAI,CAAC,SAAS,EAAE;YACd,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,oBAAoB,GAAG,IAAI,GAAG,GAAG,CAAC,CAAC;SAClD;QAED,QAAQ,GAAG,SAAS,CAAC,IAAI,CAAC,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,CAAC;QAE1C,IAAI,QAAQ,KAAK,CAAC,CAAC,IAAI,QAAQ,GAAG,GAAG,EAAE;YACrC,GAAG,GAAG,QAAQ,CAAC;SAChB;KACF;IAED,QAAQ,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC;IAC/B,GAAG,GAAG,GAAG,CAAC,GAAG,EAAE,CAAC;IAEhB,IAAI,CAAC,MAAM,CAAC,QAAQ,EAAE,GAAG,EAAE,UAAU,OAAO,EAAE,QAAQ,EAAE,MAAM;QAC5D,GAAG,CAAC,MAAM,IAAI,OAAO,CAAC,CAAC;YACrB,IAAI,EAAE,MAAM;YACZ,KAAK,EAAE,OAAO;SACf,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC","sourcesContent":["'use strict';\n\nmodule.exports = text;\n\nfunction text(eat, value, silent) {\n  var self = this;\n  var methods;\n  var tokenizers;\n  var index;\n  var length;\n  var subvalue;\n  var position;\n  var tokenizer;\n  var name;\n  var min;\n  var now;\n\n  /* istanbul ignore if - never used (yet) */\n  if (silent) {\n    return true;\n  }\n\n  methods = self.inlineMethods;\n  length = methods.length;\n  tokenizers = self.inlineTokenizers;\n  index = -1;\n  min = value.length;\n\n  while (++index < length) {\n    name = methods[index];\n\n    if (name === 'text' || !tokenizers[name]) {\n      continue;\n    }\n\n    tokenizer = tokenizers[name].locator;\n\n    if (!tokenizer) {\n      eat.file.fail('Missing locator: `' + name + '`');\n    }\n\n    position = tokenizer.call(self, value, 1);\n\n    if (position !== -1 && position < min) {\n      min = position;\n    }\n  }\n\n  subvalue = value.slice(0, min);\n  now = eat.now();\n\n  self.decode(subvalue, now, function (content, position, source) {\n    eat(source || content)({\n      type: 'text',\n      value: content\n    });\n  });\n}\n"]}]}