{"remainingRequest":"D:\\project\\Deecamp-project\\system\\Web\\node_modules\\ts-loader\\index.js??ref--4-1!D:\\project\\Deecamp-project\\system\\Web\\node_modules\\remark-parse\\lib\\tokenize\\text.js","dependencies":[{"path":"D:\\project\\Deecamp-project\\system\\Web\\node_modules\\remark-parse\\lib\\tokenize\\text.js","mtime":1516191690000},{"path":"D:\\project\\Deecamp-project\\system\\Web\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1595520254649},{"path":"D:\\project\\Deecamp-project\\system\\Web\\node_modules\\ts-loader\\index.js","mtime":1537906253000}],"contextDependencies":[],"result":["'use strict';\r\nmodule.exports = text;\r\nfunction text(eat, value, silent) {\r\n    var self = this;\r\n    var methods;\r\n    var tokenizers;\r\n    var index;\r\n    var length;\r\n    var subvalue;\r\n    var position;\r\n    var tokenizer;\r\n    var name;\r\n    var min;\r\n    var now;\r\n    if (silent) {\r\n        return true;\r\n    }\r\n    methods = self.inlineMethods;\r\n    length = methods.length;\r\n    tokenizers = self.inlineTokenizers;\r\n    index = -1;\r\n    min = value.length;\r\n    while (++index < length) {\r\n        name = methods[index];\r\n        if (name === 'text' || !tokenizers[name]) {\r\n            continue;\r\n        }\r\n        tokenizer = tokenizers[name].locator;\r\n        if (!tokenizer) {\r\n            eat.file.fail('Missing locator: `' + name + '`');\r\n        }\r\n        position = tokenizer.call(self, value, 1);\r\n        if (position !== -1 && position < min) {\r\n            min = position;\r\n        }\r\n    }\r\n    subvalue = value.slice(0, min);\r\n    now = eat.now();\r\n    self.decode(subvalue, now, function (content, position, source) {\r\n        eat(source || content)({\r\n            type: 'text',\r\n            value: content\r\n        });\r\n    });\r\n}\r\n",{"version":3,"file":"D:\\project\\Deecamp-project\\system\\Web\\node_modules\\remark-parse\\lib\\tokenize\\text.js","sourceRoot":"","sources":["D:\\project\\Deecamp-project\\system\\Web\\node_modules\\remark-parse\\lib\\tokenize\\text.js"],"names":[],"mappings":"AAAA,YAAY,CAAC;AAEb,MAAM,CAAC,OAAO,GAAG,IAAI,CAAC;AAEtB,SAAS,IAAI,CAAC,GAAG,EAAE,KAAK,EAAE,MAAM;IAC9B,IAAI,IAAI,GAAG,IAAI,CAAC;IAChB,IAAI,OAAO,CAAC;IACZ,IAAI,UAAU,CAAC;IACf,IAAI,KAAK,CAAC;IACV,IAAI,MAAM,CAAC;IACX,IAAI,QAAQ,CAAC;IACb,IAAI,QAAQ,CAAC;IACb,IAAI,SAAS,CAAC;IACd,IAAI,IAAI,CAAC;IACT,IAAI,GAAG,CAAC;IACR,IAAI,GAAG,CAAC;IAGR,IAAI,MAAM,EAAE;QACV,OAAO,IAAI,CAAC;KACb;IAED,OAAO,GAAG,IAAI,CAAC,aAAa,CAAC;IAC7B,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC;IACxB,UAAU,GAAG,IAAI,CAAC,gBAAgB,CAAC;IACnC,KAAK,GAAG,CAAC,CAAC,CAAC;IACX,GAAG,GAAG,KAAK,CAAC,MAAM,CAAC;IAEnB,OAAO,EAAE,KAAK,GAAG,MAAM,EAAE;QACvB,IAAI,GAAG,OAAO,CAAC,KAAK,CAAC,CAAC;QAEtB,IAAI,IAAI,KAAK,MAAM,IAAI,CAAC,UAAU,CAAC,IAAI,CAAC,EAAE;YACxC,SAAS;SACV;QAED,SAAS,GAAG,UAAU,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC;QAErC,IAAI,CAAC,SAAS,EAAE;YACd,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,oBAAoB,GAAG,IAAI,GAAG,GAAG,CAAC,CAAC;SAClD;QAED,QAAQ,GAAG,SAAS,CAAC,IAAI,CAAC,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,CAAC;QAE1C,IAAI,QAAQ,KAAK,CAAC,CAAC,IAAI,QAAQ,GAAG,GAAG,EAAE;YACrC,GAAG,GAAG,QAAQ,CAAC;SAChB;KACF;IAED,QAAQ,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC;IAC/B,GAAG,GAAG,GAAG,CAAC,GAAG,EAAE,CAAC;IAEhB,IAAI,CAAC,MAAM,CAAC,QAAQ,EAAE,GAAG,EAAE,UAAU,OAAO,EAAE,QAAQ,EAAE,MAAM;QAC5D,GAAG,CAAC,MAAM,IAAI,OAAO,CAAC,CAAC;YACrB,IAAI,EAAE,MAAM;YACZ,KAAK,EAAE,OAAO;SACf,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC","sourcesContent":["'use strict';\n\nmodule.exports = text;\n\nfunction text(eat, value, silent) {\n  var self = this;\n  var methods;\n  var tokenizers;\n  var index;\n  var length;\n  var subvalue;\n  var position;\n  var tokenizer;\n  var name;\n  var min;\n  var now;\n\n  /* istanbul ignore if - never used (yet) */\n  if (silent) {\n    return true;\n  }\n\n  methods = self.inlineMethods;\n  length = methods.length;\n  tokenizers = self.inlineTokenizers;\n  index = -1;\n  min = value.length;\n\n  while (++index < length) {\n    name = methods[index];\n\n    if (name === 'text' || !tokenizers[name]) {\n      continue;\n    }\n\n    tokenizer = tokenizers[name].locator;\n\n    if (!tokenizer) {\n      eat.file.fail('Missing locator: `' + name + '`');\n    }\n\n    position = tokenizer.call(self, value, 1);\n\n    if (position !== -1 && position < min) {\n      min = position;\n    }\n  }\n\n  subvalue = value.slice(0, min);\n  now = eat.now();\n\n  self.decode(subvalue, now, function (content, position, source) {\n    eat(source || content)({\n      type: 'text',\n      value: content\n    });\n  });\n}\n"]}]}